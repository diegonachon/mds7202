{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e37cb69cb73a49c2ad07cf670e073cb7",
    "deepnote_cell_height": 156.390625,
    "deepnote_cell_type": "markdown",
    "id": "XUZ1dFPHzAHl"
   },
   "source": [
    "# Laboratorio 4: Spark y EDA \n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci贸n Cient铆fica para Ciencia de Datos</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkEUN6c8S-E_"
   },
   "source": [
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti谩n Tinoco\n",
    "- Auxiliar: Catherine Benavides\n",
    "- Ayudante: Nicol谩s Ojeda, Eduardo Moya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8ebcb0f2f70c43319279fdd28c13fe89",
    "deepnote_cell_height": 171.796875,
    "deepnote_cell_type": "markdown",
    "id": "tXflExjqzAHr"
   },
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser谩n revisados\n",
    "\n",
    "- Nombre de alumno 1: Diego Cornejo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "290822720f3e4484b09e762655bcdb76",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "id": "AD-V0bbZzAHr"
   },
   "source": [
    "### **Link de repositorio de GitHub:** `https://github.com/diegonachon/mds7202/tree/main/lab4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "60255b81ff0349ad9b18f598a8d71386",
    "deepnote_cell_height": 216,
    "deepnote_cell_type": "markdown",
    "id": "hnYD2hBMAwXf",
    "tags": []
   },
   "source": [
    "### Reglas:\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Fecha de entrega: 7 d铆as desde la publicaci贸n, 3 d铆as de atraso con 1 punto de descuento c/u.\n",
    "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria.\n",
    "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser谩 debidamente penalizado con el reglamento de la escuela.\n",
    "- Tienen que subir el laboratorio a u-cursos Y a su repositorio de github. Labs que no est茅n en u-cursos no ser谩n revisados. Recuerden que el repositorio tambi茅n tiene nota.\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser谩n respondidos por este medio.\n",
    "- Pueden usar cualquer material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5bf6f5f66dcd4da9a6926774cec108ab",
    "deepnote_cell_height": 114.390625,
    "deepnote_cell_type": "markdown",
    "id": "xzz695obAwXg",
    "tags": []
   },
   "source": [
    "### Temas a tratar\n",
    "\n",
    "- Introducci贸n al manejo de datos tabulares por medio de la libreria `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "50ec30f08f2548a29bc979ed1741f5a0",
    "deepnote_cell_height": 243.390625,
    "deepnote_cell_type": "markdown",
    "id": "6uBLPj1PzAHs"
   },
   "source": [
    "### Objetivos principales del laboratorio\n",
    "\n",
    "- Entender, aplicar y aprovechar las ventajas que nos ofrece la libreria `pyspark` para manejar datos tabulares de gran vol煤men.\n",
    "- Crear gr谩ficos para el desarrollo de An谩lisis de Datos Exploratorios (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7hHEyTgm12s"
   },
   "source": [
    "### Datos del Lab\n",
    "\n",
    "- Base de datos: https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/datos_lab_spark.parquet\n",
    "- Objeto serializado: https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/object.pkl?inline=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CrDdk5NRAKe"
   },
   "source": [
    "## Preguntas Te贸ricas [12 puntos]\n",
    "(2 por pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmDMGUTxLp7M"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://img.buzzfeed.com/buzzfeed-static/static/2018-08/1/17/enhanced/buzzfeed-prod-web-05/anigif_enhanced-9173-1533160033-1.gif\" width=350 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGZZcxMWRdIa"
   },
   "source": [
    "Responda en  m谩ximo 5 l铆neas las siguientes preguntas:\n",
    "1. 驴Qu茅 es Apache Spark y cu谩les son sus principales ventajas sobre Pandas?\n",
    "\n",
    "2. 驴Qu茅 es un RDD en Spark? Describe una de sus principales caracter铆sticas. 驴Qu茅 tienen que ver con los dataframes?.\n",
    "    \n",
    "3. Diferencia entre transformaciones y acciones en Spark. Proporciona ejemplos de cada una. 驴Qu茅 ocurre internamente cuando se ejecuta una acci贸n?\n",
    "\n",
    "4. Explica la importancia del particionamiento en Spark y c贸mo afecta el rendimiento del procesamiento de datos.\n",
    "\n",
    "5. 驴Cu谩les son las funciones de Spark Driver y Spark Executor?\n",
    "\n",
    "6. 驴Qu茅 es el Catalyst Optimizer en Apache Spark y cu谩l es su funci贸n principal en la optimizaci贸n de consultas SQL?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1elJgE8JRn2O"
   },
   "source": [
    "**Respuestas**\n",
    "\n",
    "    1. Corresponde a un motor unificado, dise帽ado para el procesamiento de datos a gran escala. Las principales ventajas que tiene sobre pandas se basan en su gesti贸n de memoria, desempe帽o con grandes volumenes de datos y la capacidad de operaciones paralelas; ya que posee mayor velocidad, debido a la maximizaci贸n de recursos que posee, la construcci贸n de c谩lculos como DAGs, una estructura de datos RDD, es compatible con multiples lenguajes y es extensible, que se enfoca en el paralelismo para un procesamiento veloz.\n",
    "    \n",
    "    2. Un RDD crresponde a una colecci贸n distribuida e inmutable de elementos que peuden ser operados en paralelo. Entre sus principales caracteristicas se encuentran dependencias, que mantiene un registro de las transformaciones aplicadas a los datos de forma DAG, lo que permite a Spark reconstruirlos en caso de fallo; particiones, que como dice el nombre, divide los datos en aprticiones que permite las operaciones en paralelo; y c贸mputo de funciones, donde cada partici贸n se procesa mediante una funci贸n definida. Su relaci贸n con los dataframes viene en que estos est谩n construidos sobre los RDDs, proporcionando una interfaz m谩s amigable y optimizada para el procesamiento de datos estructurados.\n",
    "    \n",
    "    3. Las transformaciones corresponden a operaciones que crean un nuevo dataset a partir de uno existente; mientras que las acciones son operaciones que desencadenan la ejecuci贸n de todas las transformaciones acumuladas. Algunos ejemplos de transformaciones son orderBy(), groupBy(), filter() y select(); mientras que de acciones son show(), take(), count() y collect(). Cuando se ejecuta una acci贸n, se tiene que Spark optimiza el plan de ejecuci贸n de las transformaciones que deben ser ejecutadas si el resultado es necesario para una acci贸n.\n",
    "    \n",
    "    4. La importancia del particionamiento es clave para la optimizaci贸n del rendimiento, ya que propuebe el paralelismo. Esto es porque, al distribuir los datos en particiones, se facilita que los ejecutores procesen datos que se encuentren m谩s pr贸ximos a ellos, reduciendo el ancho de banda necesario para la transferencia de datos; maximizando la eficiancia del procesamiento y minimizando el tiempo de ejecuci贸n.\n",
    "\n",
    "    5. Spark driver es el encargado de instanciar la SparkSession, teniendo como principales funciones el comunicarse con el gesto del cl煤ster para solicitar recursos y transformar las operaciones definidas en los DAGs. Por su parte, Spark Executor opera en cada nodo operativo dentro del cl煤stor, comunic谩ndose con el programa conductor; siendo el encargado de llevar a cabo las tareas asignadas en los nodos, por medio del procesamiento de datos y devolviendo los resultados al conductor.\n",
    "    \n",
    "    6. Catalyst Optimizer es una parte de Apache Spark que aprovecha caracter铆sticas avanzadas del lenguaje de programaci贸n, para crear un optimizador de consultar extensible, teniendo como principales prop贸sitos el poder agregar de manera f谩cil nuevas 麓tenicas y funciones de optimizaci贸n a Spark SQL; y permitir que desarrolladores externos amplien el optimizador, por ejemplo, agregando reglas espec铆ficas de fuentes de datos, soporte de nuevos tipos de datos, entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-bf13ea5a-d8bf-4cee-879e-ba1c7035e657",
    "deepnote_cell_type": "markdown",
    "id": "b020ce37"
   },
   "source": [
    "## Parte Pr谩ctica\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0DaDvtgEYTV"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://pbs.twimg.com/ad_img/1285681293590749189/kDckYy6Z?format=png&name=900x900\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW1dg_5_WR8S"
   },
   "source": [
    "Juan Carlos Bodoque, el famoso periodista y empresario, decidi贸 diversificar su portafolio de negocios y crear su propia plataforma de e-commerce. Despu茅s de varios a帽os de investigar y analizar el mercado financiero, finalmente logr贸 fundar Bodoque E-Shop con el objetivo de ofrecer a sus clientes una experiencia personalizada y confiable en sus transacciones.\n",
    "\n",
    "Sin embargo, con la llegada de los aliens al planeta Tierra, aparecen nuevos desaf铆os para el negocio. Por ello, Bodoque decide invertir en un equipo de expertos en tecnolog铆a y comercio interplanetario, para que Bodoque Shop implemente las 煤ltimas innovaciones en servicio al cliente para garantizar la satisfacci贸n y fidelizaci贸n de sus nuevos clientes.\n",
    "\n",
    "El primer objetivo de Bodoque E-Shop ser谩 la hacer un an谩lisis exploratorio para entender mejor el comportamiento de los usuarios en la plataforma. Para ello Bodoque les hace entrega de un extenso dataset en el que se registran las actividades que han realizado sus clientes durante los 煤ltimos meses. A continuaci贸n se presenta un diccionario de variables que levanto el equipo de consultores interplanetarios de Bodoque:\n",
    "\n",
    "1. `Transaction ID`: A unique identifier for each transaction.\n",
    "2. `Customer ID`: A unique identifier for each customer.\n",
    "3. `Transaction Amount`: The total amount of money exchanged in the transaction in USD.\n",
    "4. `Transaction Date`: The date and time when the transaction took place.\n",
    "5. `Payment Method`: The method used to complete the transaction (e.g., credit card, PayPal, etc.).\n",
    "6. `Product Category`: The category of the product involved in the transaction.\n",
    "7. `Quantity`: The number of products involved in the transaction.\n",
    "8. `Customer Age`: The age of the customer making the transaction.\n",
    "9. `Customer Location`: The geographical location of the customer.\n",
    "10. `Device Used`: The type of device used to make the transaction (e.g., mobile, desktop).\n",
    "11. `IP Address`: The IP address of the device used for the transaction.\n",
    "Shipping Address: The address where the product was shipped.\n",
    "12. `Billing Address`: The address associated with the payment method.\n",
    "13. `Is An Alien`: A binary indicator of whether customer is an alien.\n",
    "14. `Account Age Days`: The age of the customer's account in days at the time of the transaction.\n",
    "15. `Transaction Hour`: The hour of the day when the transaction occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1769820f70244385ab5ac51f7509b6de",
    "deepnote_cell_height": 61.133331298828125,
    "deepnote_cell_type": "markdown",
    "id": "MhISwri4zAHy"
   },
   "source": [
    "### Importamos librerias utiles y cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xHoq7VBlJoS3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.8/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.8/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "# !apt-get install openjdk-8-jdk-headless \n",
    "e\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M6MKzLmPSHzY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.21.0-py3-none-any.whl (15.7 MB)\n",
      "\u001b[K     || 15.7 MB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from plotly) (20.9)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->plotly) (2.4.7)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.21.0 tenacity-8.2.3\n",
      "Requirement already satisfied: missingno in /opt/conda/lib/python3.8/site-packages (0.4.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from missingno) (3.3.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from missingno) (1.6.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (from missingno) (0.11.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from missingno) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->missingno) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->missingno) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->missingno) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->missingno) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->missingno) (8.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->missingno) (1.15.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn->missingno) (1.2.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn->missingno) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "# Libreria Core del lab.\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "\n",
    "\n",
    "#Libreria para plotear\n",
    "!pip install --upgrade plotly\n",
    "!pip install missingno\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vJWSlEXYBqq"
   },
   "source": [
    "Cargue los datos usando **pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "i9Uf-BTZXqXe"
   },
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ab5fe6bbecc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Escriba su respuesta aqu铆\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cargar archivo Parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.master\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 raise PySparkRuntimeError(\n\u001b[0m\u001b[1;32m    108\u001b[0m                     \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"JAVA_GATEWAY_EXITED\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "# Escriba su respuesta aqu铆\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Cargar archivo Parquet\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.read.parquet(\"datos_lab_spark.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6l6GNynYnh4"
   },
   "source": [
    "### 2. Limpieza con pyspark [8 puntos]\n",
    "(1 punto por pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DVdjYyOGRom"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:600/1*A6PpTrehGLxCJWNcUsDTNg.jpeg\" width=350 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPGV40BjZekP"
   },
   "source": [
    "Para comenzar con el an谩lisis exploratorio usted decide empezar limpiando la base de datos con **pyspark** dado el alto volumen de datos que genera diariamente Bodoque E-Shop.\n",
    "\n",
    "**Nota: NO SE PERMITE EL USO DE PANDAS EN ESTA SECCIN**\n",
    "\n",
    "\n",
    "\n",
    "1.   Utilice `.printSchema()` para revisar la estructura de los datos\n",
    "2.   Muestre las primeras 10 filas del dataset. Hint: utilice `.show()`\n",
    "3.   Imprima un muestreo aleatorio con el 5% de los datos diponibles. . Hint: utilice `.sample()`\n",
    "4. Revise los tipos de datos de cada columna con `.dtypes()` y responda la siguiente pregunta: 驴Cu谩l/es columna/s tiene/n un tipo de dato que no es el adecuado y por qu茅?\n",
    "5. Complete el c贸digo entregado para cambiar el tipo de datos para la/s columna/s problem谩ticas.\n",
    "6. Cuente la cantidad de datos nulos por variable. Recuerde que Spark no posee un m茅todo que le permita calcular directamente los nulos.\n",
    "7. Elimine datos nulos.\n",
    "8. Elimine datos duplicados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nw95Jvr-DtwS"
   },
   "outputs": [],
   "source": [
    "# Escriba su respuesta aqu铆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjxI2Xd6cRu1"
   },
   "source": [
    "### 3. Transformaciones con pyspark [6 puntos]\n",
    "(1 punto por pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPfhWPZeHXUH"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://live.staticflickr.com/13/91801406_0e71d7f019_b.jpg\" width=350 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbIDKn44cWhI"
   },
   "source": [
    "**Nota: NO SE PERMITE EL USO DE PANDAS EN ESTA SECCIN**\n",
    "\n",
    "Para continuar con el an谩lisis, los especistas de Bodoque les gustar铆a tener nuevas variables disponibles. Tras las notas de la reuni贸n usted llega a la conclusi贸n de que tiene que realizar las siguientes tareas (con el dataset preprocesado de la seccion anterior):\n",
    "\n",
    "\n",
    "1.   Agregar una columna llamada \"Transaction bp\" con el **monto total** de la transacci贸n en bodoque pesos. Se considera que $x$ d贸lares equivalen a $log(48+|x^{36}|)$ bodoque pesos.\n",
    "2.   Crear una columna llamada \"Transaction Month\" con el mes en que se realiza una transacci贸n.\n",
    "3.   Crear la variable *Type of purchase* seg煤n la catidad de unidades vendidas de acuerdo a las siguientes categor铆as.\n",
    "  * Compra minorista: 5 productos o menos.\n",
    "  * Compra mayorista: 6 produtos o m谩s.\n",
    "4. Imprima los registros de compras hechas por alien铆genas en el comecio mayorista.  Utilice `.filter()`.\n",
    "5. Cuente la cantidad de compras realizadas por humanos y la cantidad de compras realizadas por alien铆genas. Utilice `.groupby()`.\n",
    "6. Muestre una tabla con la recaudaci贸n promedio por transacci贸n para cada m茅todo de pago, tanto para humanos como alien铆genas. Utilice `pivot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbtFJi3mHnkK"
   },
   "outputs": [],
   "source": [
    "# Escriba su respuesta aqu铆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17Muj6u2jOLq"
   },
   "source": [
    "### 4. EDA [18 puntos]\n",
    "(1 punto por gr谩fico y 1 punto por su interpretaci贸n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F3yo66wFQ0z"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://i.pinimg.com/originals/41/7e/7b/417e7b9089bcc20c4909df8954c6e742.gif\" width=400 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayN5LYRamE7-"
   },
   "source": [
    "Esta secci贸n tiene como objetivo evaluar su habilidad para generar reportes y conclusiones a partir de los patrones identificados en los datos proporcionados por Bodoque. Espec铆ficamente, se enfoca en **caracterizar las transacciones** y **explorar las diferencias y similitudes en el comportamiento de humanos y aliens**. Utilice el dataset que ya incluye las transformaciones necesarias.\n",
    "\n",
    "Por favor, aseg煤rese de que **todas** las visualizaciones que realice cumplan con los siguientes criterios:\n",
    "- Deben ser relevantes y f谩ciles de interpretar.\n",
    "- Cada gr谩fico debe incluir un t铆tulo claro, nombres en los ejes y leyendas adecuadas.\n",
    "- Adjunte una breve descripci贸n interpretativa junto a cada gr谩fico para explicar los resultados visualizados.\n",
    "\n",
    "Para llevar a cabo esta tarea, siga los siguientes pasos utilizando la librer铆a de visualizaci贸n de su elecci贸n (matplotlib, seaborn, plotly, etc):\n",
    "\n",
    "1. **Conversi贸n del DataFrame a formato pandas**: Pase el DataFrame procesado a formato pandas. Evite realizar transformaciones adicionales con pandas.\n",
    "2. **Visualizaci贸n de Variables Categ贸ricas**:\n",
    "   - Genere **tres gr谩ficos de barras** que diferencien entre humanos y aliens. Analice y comente cualquier diferencia o similitud observada entre estos dos grupos.\n",
    "3. **Visualizaci贸n de Variables Num茅ricas**:\n",
    "   - Elabore **tres distplots** para examinar las distribuciones de variables num茅ricas, diferenciando entre humanos y aliens. Comente las diferencias o similitudes notables.\n",
    "4. **An谩lisis de Patrones en Transacciones**:\n",
    "   - Cree **tres gr谩ficos avanzados** que ayuden a identificar patrones en las transacciones. Estos gr谩ficos deben incorporar al menos dos dimensiones y diferir de los anteriores. Algunos ejemplos podr铆an ser un lineplot que muestre la cantidad de transacciones mensuales por canal de venta, o un barplot que exhiba los tres productos m谩s vendidos por canal.\n",
    "\n",
    "Estos pasos le permitir谩n no solo visualizar datos complejos de manera efectiva, sino tambi茅n interpretar estos datos para extraer insights valiosos acerca del comportamiento de los consumidores en el contexto de Bodoque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGw5y36IxRk3"
   },
   "outputs": [],
   "source": [
    "# Escriba su respuesta aqu铆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97zN2_g4vgY6"
   },
   "source": [
    "### 5. Particiones y consultas en SQL [6 puntos]\n",
    "(2 puntos por pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viNvNuE_odgc"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/1696330143457.gif\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCdHwyGBwVx8"
   },
   "source": [
    "El equipo de Bodoque e-shop ha solicitado que los datos est茅n disponibles en una tabla SQL consultable. Adem谩s, est谩n interesados en aprovechar las funciones de ventana en SQL para an谩lisis avanzados. Las funciones de ventana permiten realizar c谩lculos sobre un conjunto de filas que est谩n relacionadas con la fila actual. Por ejemplo, UNBOUNDED PRECEDING se usa para indicar que el rango de la funci贸n de ventana comienza desde la primera fila de la partici贸n o del conjunto de resultados, lo cual es 煤til para calcular sumas acumulativas hasta la fila actual. Las variaciones comunes de este uso incluyen:\n",
    "\n",
    "- `UNBOUNDED PRECEDING` to `CURRENT ROW`: Calcula desde el inicio de la partici贸n hasta la fila actual.\n",
    "- `UNBOUNDED PRECEDING` to `UNBOUNDED FOLLOWING`: Cubre todas las filas dentro de la partici贸n.\n",
    "- `VALUE PRECEDING` to `VALUE FOLLOWING`: Establece un rango espec铆fico basado en valores antes y despu茅s de la fila actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VntjejKLleIa"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://learnsql.com/blog/sql-window-functions-rows-clause/1.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8XJ7NrPllKG"
   },
   "source": [
    "Ejemplo de uso en SQL:\n",
    "\n",
    "```sql\n",
    "STAT(COL1_NAME) OVER (PARTITION BY COL2_NAME ORDER BY COL3_NAME ROWS BETWEEN X PRECEDING AND CURRENT ROW)\n",
    "```\n",
    "\n",
    "\n",
    "Responda y realice los siguientes puntos:\n",
    "\n",
    "1. **Creaci贸n de Tabla con PySpark**:\n",
    "   - Desarrolle un script utilizando PySpark para crear una tabla a partir de un DataFrame previamente transformado. Seleccione y utilice una variable espec铆fica para la partici贸n de la tabla. Justifique su elecci贸n de esta variable considerando factores como el tama帽o del DataFrame, la distribuci贸n de los datos y el impacto potencial en el rendimiento de futuras consultas.\n",
    "\n",
    "2. **Consulta SQL para Principales Clientes**:\n",
    "   - Ejecute una consulta SQL para identificar los 10 clientes que m谩s productos han comprado. La consulta debe retornar el ID del cliente junto con el total de productos comprados, ordenados en forma descendente.\n",
    "\n",
    "3. **Implementaci贸n de Funci贸n de Ventana en SQL y Equivalente en Spark**:\n",
    "   - Implemente una funci贸n de ventana en SQL para calcular la compra m谩s alta realizada por cada usuario en los 煤ltimos tres meses. Adem谩s, describa c贸mo se podr铆a realizar una funci贸n equivalente en Spark, considerando las capacidades espec铆ficas de PySpark para manejar este tipo de consultas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe_JQ3npiM6_"
   },
   "outputs": [],
   "source": [
    "# C贸digo Aqu铆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKQs-augfZBv"
   },
   "source": [
    "### 6. UDF [10 puntos]\n",
    "(2 por pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovDBGi-uhhdD"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://64.media.tumblr.com/ba8c705edd2bed0a28d9458811155d69/tumblr_pap19zg4ae1w3zg6go1_400.gifv\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJUUnpi8qKHD"
   },
   "source": [
    "\n",
    "\n",
    "Un experto en predicciones y programaci贸n le ha proporcionado un objeto serializado (`pickle`) dise帽ado para calcular las probabilidades de que un cliente cometa o no un fraude. Este experto sugiere que, para maximizar las capacidades de procesamiento distribuido de Spark, deber铆a implementar `Scalar User Defined Functions` (udf). Esto le permitir谩 aplicar el objeto serializado en un entorno distribuido a lo largo de toda la poblaci贸n de datos. Un aspecto clave de la funci贸n desarrollada por el experto es que se enfoca exclusivamente en las siguientes columnas para realizar las predicciones: `['Transaction Amount', 'Quantity', 'Customer Age', 'Transaction Hour']`.\n",
    "\n",
    "Aparte, el experto le proporciona las siguientes instrucciones para usar las UDF en Spark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def custom_function(col):\n",
    "    pass\n",
    "\n",
    "udf_function = udf(custom_function, FloatType())\n",
    "```\n",
    "\n",
    "Bas谩ndose en la estructura proporcionada, debe desarrollar una funci贸n que ejecute un c贸digo espec铆fico. Tenga en cuenta que esta funci贸n solo puede recibir columnas de Spark y debe retornar el valor deseado. Posteriormente, deber谩 utilizar esta funci贸n UDF indicando la funci贸n personalizada y el formato de salida.\n",
    "\n",
    "Siga los siguientes pasos para implementar la soluci贸n y responda las preguntas:\n",
    "\n",
    "1. **Cargar el objeto serializado**: Revise el tipo de objeto y deduzca su funci贸n.\n",
    "2. **Explorar el objeto**: Utilice las funciones `dir` y `help` para identificar qu茅 m茅todo del objeto predice la probabilidad.\n",
    "3. **Crear una funci贸n personalizada**: Elabore una funci贸n que prediga la probabilidad de fraude utilizando el 煤ltimo valor de la lista generada por el objeto serializado. Puede modificar el nombre de la funci贸n para reflejar su prop贸sito.\n",
    "4. **Definir la funci贸n UDF**: Establezca la funci贸n UDF con la funci贸n personalizada que ha creado.\n",
    "5. **Generar una nueva columna**: A帽ada una nueva columna `prediction` a su DataFrame en Spark utilizando la funci贸n UDF y muestre un ejemplo de c贸mo se aplica. 驴Qu茅 beneficios podr铆a generar utilizar udf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPJVs2OBezN_"
   },
   "outputs": [],
   "source": [
    "# C贸digo Aqu铆"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
